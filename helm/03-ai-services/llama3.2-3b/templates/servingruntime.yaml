{{- if .Values.servingRuntime.enabled }}
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: llama3-2-3b
  labels:
    {{- include "llama3-2-3b.labels" . | nindent 4 }}
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/accelerator-name: migrated-gpu
spec:
  supportedModelFormats:
  - name: vLLM
    version: "1"
    autoSelect: true
  - name: huggingface
    version: "1"
    autoSelect: true
  containers:
  - name: kserve-container
    {{- $device := lower (.Values.device | default "gpu") }}
    {{- $image := index .Values.image $device }}
    {{- $resources := index .Values.resources $device }}
    image: "{{ (index $image "repository") }}:{{ (index $image "tag") }}"
    imagePullPolicy: {{ default "IfNotPresent" (index $image "pullPolicy") }}
    args:
    - --model=/mnt/models
    - --port=8080
    - --max-model-len={{ .Values.model.maxModelLen | default 8192 }}
    {{- if eq $device "gpu" }}
    - --gpu_memory_utilization={{ .Values.model.gpuMemoryUtilization | default 0.95 }}
    {{- end }}
    - '--served-model-name=llama3-2-3b'
    - --chat-template=/app/data/template/tool_chat_template_llama3.2_json.jinja
    - '--enable-auto-tool-choice'
    - '--tool-call-parser'
    - llama3_json
    {{- if .Values.servingRuntime.tracing.enabled }}
    # tracing-specific flags and options
    - --otlp-traces-endpoint
    - {{ .Values.servingRuntime.tracing.otlpTracesEndpoint }}
    - --collect-detailed-traces
    - {{ .Values.servingRuntime.tracing.collectDetailedTraces | quote }}
    {{- end }}
    env:
    - name: HF_HOME
      value: /tmp/hf_home
    {{- if .Values.servingRuntime.tracing.enabled }}
    - name: OTEL_SERVICE_NAME
      value: {{ .Values.servingRuntime.tracing.serviceName | quote }}
    - name: OTEL_EXPORTER_OTLP_TRACES_INSECURE
      value: {{ .Values.servingRuntime.tracing.insecure | quote }}
    {{- end }}
    {{- range $key, $value := .Values.env }}
    {{- if ne $key "HF_HOME" }}
    - name: {{ $key }}
      value: {{ $value | quote }}
    {{- end }}
    {{- end }}
    ports:
    - containerPort: 8080
      name: h2c
      protocol: TCP
    resources:
      {{- toYaml $resources | nindent 6 }}
    volumeMounts:
    - name: shm
      mountPath: /dev/shm
  volumes:
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: {{ .Values.servingRuntime.shmSizeLimit | default "1Gi" }}
  {{- if and .Values.nodeSelector (hasKey .Values.nodeSelector $device) }}
  nodeSelector:
    {{- toYaml (index .Values.nodeSelector $device) | nindent 4 }}
  {{- end }}
  {{- if and .Values.affinity (hasKey .Values.affinity $device) }}
  affinity:
    {{- toYaml (index .Values.affinity $device) | nindent 4 }}
  {{- end }}
  {{- if and .Values.tolerations (hasKey .Values.tolerations $device) }}
  tolerations:
    {{- toYaml (index .Values.tolerations $device) | nindent 4 }}
  {{- end }}
  builtInAdapter:
    serverType: vllm
    runtimeManagementPort: 8080
    memBufferBytes: {{ .Values.servingRuntime.memBufferBytes | default 134217728 }}
    modelLoadingTimeoutMillis: {{ .Values.servingRuntime.modelLoadingTimeoutMillis | default 90000 }}
{{- end }}