# Default values for llama3.2-3b
replicaCount: 1

device: "gpu"  # Options: gpu, xeon
image:
  gpu:
    # repository: 'quay.io/modh/vllm@sha256:'
    # tag: "0d55419f3d168fd80868a36ac89815dded9e063937a8409b7edf3529771383f3"
    repository: 'quay.io/rcarrata/vllm-otlp-tracing@sha256'
    tag: "16f83f5858fcc04bd56ea785126c04af823e8aacbeabb9db963f86d252178189"
    pullPolicy: IfNotPresent
  xeon:
    repository: 'docker.io/opea/vllm-cpu-ubi'
    tag: "v0.12.0-ubi9"
    pullPolicy: IfNotPresent


imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations: {}

podSecurityContext:
  runAsNonRoot: false
  fsGroup: 2000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL

# Model configuration
model:
  name: "meta-llama/Llama-3.2-3B-Instruct"
  # Alternative models:
  # name: "meta-llama/Llama-3.2-3B"
  maxModelLen: 65000
  # gpuMemoryUtilization: 0.40
  # quantization: ""  # Options: awq, gptq, squeezellm, fp8
  # dtype: "auto"     # Options: auto, half, float16, bfloat16, float32
  
# vLLM server configuration
vllm:
  host: "0.0.0.0"
  port: 8000
  apiKey: ""  # Optional API key
  enableLogging: true
  logLevel: "info"
  maxLogLen: 2048
  disableLogStats: false
  disableLogRequests: false

# Data connection for model storage
dataConnection:
  enabled: true
  name: "llama-models"
  accessKeyId: ""
  secretAccessKey: ""
  endpoint: ""
  bucket: "llama-models"
  region: "us-east-1"

service:
  type: ClusterIP
  port: 80
  targetPort: 8000
  annotations: {}

route:
  enabled: true
  annotations: {}
  host: ""
  tls:
    enabled: true
    termination: edge
    insecureEdgeTerminationPolicy: Redirect

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: llama-3-2-3b.local
      paths:
        - path: /
          pathType: Prefix
  tls: []

# Resource requirements (adjust based on GPU availability)
resources:
  limits:
    nvidia.com/gpu: 1
    memory: 24Gi
    cpu: 4
  requests:
    nvidia.com/gpu: 1
    memory: 16Gi
    cpu: 2

livenessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 120
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 80

nodeSelector:
  nvidia.com/gpu.present: "true"

tolerations:
  - effect: NoSchedule
    operator: Exists

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: nvidia.com/gpu.present
          operator: In
          values:
          - "true"

# Persistent Volume for model cache
persistence:
  enabled: true
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 50Gi
  mountPath: /root/.cache

# Environment variables
env:
  CUDA_VISIBLE_DEVICES: "0"
  TRANSFORMERS_CACHE: "/root/.cache/huggingface"
  HF_HOME: "/root/.cache/huggingface"

# InferenceService configuration
inferenceService:
  displayName: "llama3.2-3b"
  maxReplicas: 1
  minReplicas: 1
  modelFormat: "vLLM"
  modelName: ""
  runtime: ""  # Will default to chart fullname if not specified
  storageUri: "oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct"

# Serving Runtime configuration
servingRuntime:
  enabled: true  # Set to true to create a ServingRuntime resource
  image: "quay.io/modh/vllm@sha256:0d55419f3d168fd80868a36ac89815dded9e063937a8409b7edf3529771383f3"
  tensorParallelSize: 1
  shmSizeLimit: "1Gi"
  memBufferBytes: 134217728  # 128MB
  modelLoadingTimeoutMillis: 90000  # 90 seconds
  
  # OpenTelemetry tracing configuration
  tracing:
    enabled: true
    otlpTracesEndpoint: "grpc://otel-collector-collector.observability-hub.svc.cluster.local:4317"
    collectDetailedTraces: "all"
    serviceName: "vllm-llama32b"
    insecure: true

# Network Policies
networkPolicy:
  enabled: true
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: openshift-ingress
      ports:
      - protocol: TCP
        port: 8000
    - from:
      - podSelector:
          matchLabels:
            app.kubernetes.io/name: llama-stack
      ports:
      - protocol: TCP
        port: 8000